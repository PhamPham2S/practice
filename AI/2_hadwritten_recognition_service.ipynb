{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fd6bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준 라이브러리\n",
    "import os\n",
    "import unicodedata\n",
    "from io import BytesIO\n",
    "\n",
    "# 서드파티 라이브러리\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AutoTokenizer\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c036b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97612b6b1c94da8ac1feb30c8ffde14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/364 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ebb6e279f84c7582694bd12f151bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/475 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02fae6976e284b71ba8fb4dbe7baa285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/104k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2580ff785a548158f8d7ffe710f50ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/358k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1744f0e303ad47f182528ea64354add4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/my_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e32c974ac44ec4aa5716a4a4389f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe99983523d4f83956ac54dc91c0482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/427M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지방토목주사\n"
     ]
    }
   ],
   "source": [
    "# TrOCR 모델과 관련된 전처리기, 모델, 토크나이저를 불러옴\n",
    "processor = TrOCRProcessor.from_pretrained(\"ddobokki/ko-trocr\")  # TrOCR 전처리기\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"ddobokki/ko-trocr\")  # TrOCR 비전-텍스트 모델\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ddobokki/ko-trocr\")  # TrOCR 모델의 토크나이저\n",
    "\n",
    "# URL에서 이미지 다운로드\n",
    "url = \"https://raw.githubusercontent.com/ddobokki/ocr_img_example/master/g.jpg\"\n",
    "response = requests.get(url)  # URL로부터 이미지 다운로드\n",
    "img = Image.open(BytesIO(response.content))  # 이미지를 메모리에 로드하여 열기\n",
    "\n",
    "# 이미지 데이터를 모델 입력값으로 전처리\n",
    "pixel_values = processor(img, return_tensors=\"pt\").pixel_values  # 이미지를 모델에 맞는 텐서로 변환\n",
    "\n",
    "# 모델을 사용해 텍스트 생성 (OCR 수행)\n",
    "generated_ids = model.generate(pixel_values, max_length=64)  # 이미지로부터 텍스트 토큰 생성\n",
    "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]  # 토큰을 텍스트로 디코딩\n",
    "\n",
    "# 텍스트 정규화 (NFC 형식)\n",
    "generated_text = unicodedata.normalize(\"NFC\", generated_text)  # 유니코드 정규화로 텍스트 통일\n",
    "\n",
    "# 결과 출력\n",
    "print(generated_text)  # 추출된 텍스트 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c226f5a-9403-41ec-b23d-7288703eda26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] init TrOCR Inferencer\n"
     ]
    }
   ],
   "source": [
    "# TrOCRInferencer 클래스 정의 - 모델, 전처리기, 토크나이저를 불러와 설정\n",
    "class TrOCRInferencer:\n",
    "    def __init__(self):\n",
    "        # 초기화 시, 모델과 전처리기, 토크나이저를 미리 학습된 모델에서 불러옴\n",
    "        print(\"[info] init TrOCR Inferencer\")\n",
    "        self.processor = TrOCRProcessor.from_pretrained(\"ddobokki/ko-trocr\")\n",
    "        self.model = VisionEncoderDecoderModel.from_pretrained(\"ddobokki/ko-trocr\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"ddobokki/ko-trocr\")\n",
    "        \n",
    "    # 이미지에서 텍스트를 추출하는 메서드\n",
    "    def inference(self, image):\n",
    "        # 이미지를 텐서 형식으로 전처리\n",
    "        pixel_values = self.processor(images=image, return_tensors='pt').pixel_values\n",
    "        # 모델을 사용해 텍스트 토큰 생성\n",
    "        generated_ids = self.model.generate(pixel_values, max_length=64)\n",
    "        # 토큰을 실제 텍스트로 디코딩\n",
    "        generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        # 유니코드 정규화로 텍스트 정리\n",
    "        generated_text = unicodedata.normalize(\"NFC\", generated_text)\n",
    "        \n",
    "        return generated_text\n",
    "\n",
    "# TrOCRInferencer 객체 생성\n",
    "inferencer = TrOCRInferencer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f66301dc-aee6-4ebb-8758-7261308d4bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_text(image):\n",
    "    # dict 형태일 경우 composite 이미지 추출\n",
    "    if isinstance(image, dict):\n",
    "        image = image.get('composite', None)  # 'composite' 키에 접근\n",
    "        if image is None:\n",
    "            return \"No valid image data found\"\n",
    "    \n",
    "    # 이미지가 NumPy 배열일 경우 처리\n",
    "    image = Image.fromarray(image).convert('RGB')\n",
    "    \n",
    "    try:\n",
    "        # 추론을 통해 텍스트 추출\n",
    "        text = inferencer.inference(image)\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef383ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://a4bab1cafe69ff7113.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    }
   ],
   "source": [
    "brush = gr.Brush(default_size = 3)\n",
    "\n",
    "# Gradio 인터페이스 정의\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"# Handwritten Image OCR\")  # 앱 제목\n",
    "    # 첫 번째 탭: 이미지 업로드 방식\n",
    "    with gr.Tab(\"Image upload\"):\n",
    "        image = gr.Image(label=\"Handritten image file\")  # 이미지 업로드 컴포넌트\n",
    "        output = gr.Textbox(label=\"Output Box\")  # 결과 텍스트 박스\n",
    "        convert_btn = gr.Button(\"Convert\")  # 변환 버튼\n",
    "        # 버튼 클릭 시 이미지에서 텍스트로 변환하는 함수 호출\n",
    "        convert_btn.click(\n",
    "            fn=image_to_text, inputs=image, outputs=output\n",
    "        )\n",
    "        # 예시 이미지 기능 (주석 처리됨)\n",
    "        # gr.Markdown(\"## Image Examples\")\n",
    "        # gr.Examples(\n",
    "        #     examples=[\n",
    "        #         os.path.join(os.getcwd(), \"examples/Hello.png\"),\n",
    "        #         os.path.join(os.getcwd(), \"examples/Hello_cursive.png\"),\n",
    "        #         os.path.join(os.getcwd(), \"examples/Red.png\"),\n",
    "        #         os.path.join(os.getcwd(), \"examples/sentence.png\"),\n",
    "        #         os.path.join(os.getcwd(), \"examples/i_love_you.png\"),\n",
    "        #         os.path.join(os.getcwd(), \"examples/merrychristmas.png\"),\n",
    "        #         os.path.join(os.getcwd(), \"examples/Rock.png\"),\n",
    "        #         os.path.join(os.getcwd(), \"examples/Bob.png\"),\n",
    "        #     ],\n",
    "        #     inputs=image,\n",
    "        #     outputs=output,\n",
    "        #     fn=image_to_text\n",
    "        # )\n",
    "    \n",
    "    # 두 번째 탭: 스케치 패드로 손글씨 작성\n",
    "    with gr.Tab(\"Drawing\"):\n",
    "        gr.Markdown(\"# Handwritten Image OCR\")  # 탭 제목\n",
    "        # 스케치패드 컴포넌트 (사용자가 손글씨를 그림)\n",
    "        sketchpad = gr.Sketchpad(\n",
    "            label = \"Handwritten Sketchpad\",  # 라벨명 수정\n",
    "            width=600,  # 스케치패드 너비\n",
    "            height=400,  # 스케치패드 높이\n",
    "            brush=brush,\n",
    "            container=True,  # 레이아웃 크기 고정\n",
    "            scale=2,  # 크기 비율 설정\n",
    "        )\n",
    "        output = gr.Textbox(label=\"Output Box\")  # 결과 텍스트 박스\n",
    "        convert_btn = gr.Button(\"Convert\")  # 변환 버튼\n",
    "        # 버튼 클릭 시 스케치패드에서 작성한 그림을 텍스트로 변환\n",
    "        convert_btn.click(\n",
    "            fn=image_to_text, inputs=sketchpad, outputs=output\n",
    "        )\n",
    "\n",
    "app.launch(inline=False, share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
